{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8aafa09",
      "metadata": {
        "id": "a8aafa09"
      },
      "source": [
        "### Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "**Answer:**  \n",
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm used for both classification and regression.  \n",
        "- In **classification**, the algorithm assigns a class to a new data point based on the majority class of its k nearest neighbors.  \n",
        "- In **regression**, the prediction is the average (or weighted average) of the values of its k nearest neighbors.  \n",
        "\n",
        "KNN relies on distance metrics like Euclidean or Manhattan distance to find the nearest neighbors.  \n",
        "It is simple, non-parametric, and effective for smaller datasets.  \n",
        "\n",
        "**Example Code (Classification):**  \n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e054d98",
      "metadata": {
        "id": "7e054d98"
      },
      "source": [
        "### Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "**Answer:**  \n",
        "The Curse of Dimensionality refers to various problems that arise when analyzing and organizing data in high-dimensional spaces.  \n",
        "- In high dimensions, distances between points become less meaningful because all points tend to look equally far apart.  \n",
        "- For KNN, this means that finding \"nearest\" neighbors becomes difficult and noisy.  \n",
        "- As a result, model performance may degrade due to poor neighborhood structure.  \n",
        "\n",
        "**Impact on KNN:**  \n",
        "- Requires more data to achieve the same accuracy.  \n",
        "- Increases computation time.  \n",
        "- Feature scaling and dimensionality reduction (e.g., PCA) help mitigate this issue.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3c59ca",
      "metadata": {
        "id": "1f3c59ca"
      },
      "source": [
        "### Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Answer:**  \n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms original features into a smaller set of uncorrelated variables called principal components.  \n",
        "- These components capture the maximum variance in the data.  \n",
        "\n",
        "**Difference from Feature Selection:**  \n",
        "- PCA creates **new features** (linear combinations of original ones), while feature selection chooses a subset of original features.  \n",
        "- PCA focuses on variance maximization, feature selection focuses on importance.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ba65ea",
      "metadata": {
        "id": "27ba65ea"
      },
      "source": [
        "### Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "**Answer:**  \n",
        "- **Eigenvectors**: Directions along which data varies the most. They represent principal components.  \n",
        "- **Eigenvalues**: Magnitude of variance captured by each eigenvector.  \n",
        "- In PCA, eigenvalues help decide which principal components to keep (larger eigenvalues = more variance).  \n",
        "- They are crucial for dimensionality reduction because they determine how much information is preserved.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed300031",
      "metadata": {
        "id": "ed300031"
      },
      "source": [
        "### Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "**Answer:**  \n",
        "- KNN suffers in high dimensions due to the Curse of Dimensionality.  \n",
        "- PCA reduces dimensions by keeping only the most informative components.  \n",
        "- When PCA is applied before KNN:  \n",
        "  - Noise is reduced.  \n",
        "  - Computational efficiency improves.  \n",
        "  - Accuracy may improve due to better neighborhood structure.  \n",
        "Thus, PCA + KNN pipeline is robust for high-dimensional datasets.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffb0842",
      "metadata": {
        "id": "4ffb0842"
      },
      "source": [
        "### Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95b1485",
      "metadata": {
        "id": "b95b1485"
      },
      "source": [
        "### Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b9273b",
      "metadata": {
        "id": "d3b9273b"
      },
      "source": [
        "### Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "# PCA with top 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "print(\"Accuracy with PCA (2 components):\", accuracy_score(y_test, y_pred_pca))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164cce25",
      "metadata": {
        "id": "164cce25"
      },
      "source": [
        "### Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "# Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "print(\"Euclidean Accuracy:\", accuracy_score(y_test, y_pred_euclidean))\n",
        "\n",
        "# Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "print(\"Manhattan Accuracy:\", accuracy_score(y_test, y_pred_manhattan))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55559974",
      "metadata": {
        "id": "55559974"
      },
      "source": [
        "### Question 10: High-dimensional gene expression dataset classification problem\n",
        "\n",
        "**Answer:**  \n",
        "Steps to build a robust pipeline:  \n",
        "1. **Use PCA for Dimensionality Reduction**: Reduce thousands of features into fewer components while retaining maximum variance.  \n",
        "2. **Decide Number of Components**: Use explained variance ratio (keep ~95% variance).  \n",
        "3. **Apply KNN**: Train classifier on reduced dataset.  \n",
        "4. **Evaluate**: Use cross-validation, accuracy, F1-score, and confusion matrix.  \n",
        "5. **Justification**:  \n",
        "   - PCA prevents overfitting.  \n",
        "   - KNN is simple and interpretable.  \n",
        "   - Pipeline ensures scalability for real biomedical data.  \n",
        "\n",
        "**Example Code:**  \n",
        "```python\n",
        "# PCA for high-dimensional dataset\n",
        "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train_r, X_test_r, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_r, y_train)\n",
        "y_pred_r = knn.predict(X_test_r)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy after PCA + KNN:\", accuracy_score(y_test, y_pred_r))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}