{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17317439",
   "metadata": {},
   "source": [
    "### Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
    "\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates the data into different classes with the maximum margin. The margin is defined as the distance between the hyperplane and the closest data points from each class, known as support vectors. The idea is to choose a hyperplane that maximizes this margin to ensure better generalization on unseen data.\n",
    "\n",
    "In the case of non-linearly separable data, SVM uses kernel functions to map the data into a higher-dimensional space where a linear separation is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a9d10",
   "metadata": {},
   "source": [
    "### Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
    "\n",
    "- **Hard Margin SVM** assumes that the data is linearly separable and tries to find a hyperplane that perfectly separates the data without any misclassification. It works only when there are no noisy points or overlaps in the classes.\n",
    "- **Soft Margin SVM**, on the other hand, allows some misclassification (slack) to find a better generalizing hyperplane. It introduces a regularization parameter `C` which balances the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "Soft Margin SVM is more practical and is widely used in real-world problems where perfect separation is not always possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a4855",
   "metadata": {},
   "source": [
    "### Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
    "\n",
    "The Kernel Trick is a technique used in SVM to handle non-linearly separable data. It transforms the data into a higher-dimensional space where a linear separator (hyperplane) can be found.\n",
    "\n",
    "Instead of explicitly computing the transformation, the kernel trick computes the inner product of the transformed features directly using a kernel function.\n",
    "\n",
    "**Example: Radial Basis Function (RBF) Kernel** - It is useful when the decision boundary between classes is non-linear and complex. RBF kernel can capture intricate patterns in the data by projecting it into an infinite-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cdd8ad",
   "metadata": {},
   "source": [
    "### Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
    "\n",
    "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It predicts the class of a data point by calculating the posterior probability for each class and selecting the one with the highest probability.\n",
    "\n",
    "It is called “naïve” because it assumes that all features are independent of each other given the class label. This assumption is rarely true in real-world data, but the classifier still performs well in many cases due to its simplicity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644c51f",
   "metadata": {},
   "source": [
    "### Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
    "\n",
    "- **Gaussian Naïve Bayes**: Assumes features follow a normal distribution. Used when features are continuous (e.g., real-valued attributes like height, weight).\n",
    "- **Multinomial Naïve Bayes**: Used for discrete count data such as word counts in text classification. Ideal for bag-of-words models.\n",
    "- **Bernoulli Naïve Bayes**: Used for binary/boolean features indicating the presence or absence of a feature (e.g., whether a word appears in a document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60f170",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 2 – SVM & Naive Bayes (Coding Part)\n",
    "\n",
    "*Solutions with code, output, and explanations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad159dc6",
   "metadata": {},
   "source": [
    "### Question 6: Train SVM on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ce775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "support_vectors = model.support_vectors_\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Support Vectors:\")\n",
    "print(support_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d164a87",
   "metadata": {},
   "source": [
    "### Question 7: Gaussian Naive Bayes on Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train GaussianNB model\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bc217",
   "metadata": {},
   "source": [
    "### Question 8: GridSearchCV with SVM on Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load Wine dataset\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Output best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe99d3",
   "metadata": {},
   "source": [
    "### Question 9: Naive Bayes on Text Dataset with ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load text dataset\n",
    "data = fetch_20newsgroups(subset='train', categories=['rec.sport.hockey', 'sci.med'], remove=('headers', 'footers', 'quotes'))\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Pipeline with TF-IDF + Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(X, y)\n",
    "y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1fa18",
   "metadata": {},
   "source": [
    "### Question 10: Email Spam Classification Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03919e",
   "metadata": {},
   "source": [
    "\n",
    "To classify emails as Spam or Not Spam, here's the complete approach:\n",
    "\n",
    "**Preprocessing:**\n",
    "- Use TfidfVectorizer to convert text into numerical format.\n",
    "- Handle missing data using SimpleImputer or fill with empty strings.\n",
    "\n",
    "**Model Choice:**\n",
    "- Use **Multinomial Naive Bayes** if speed and scalability are priority.\n",
    "- Use **SVM** if higher accuracy is desired but at higher computation cost.\n",
    "- Naive Bayes is preferred for sparse, high-dimensional text data.\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- Use `class_weight='balanced'` in SVM or SMOTE oversampling.\n",
    "- Use stratified split to maintain class distribution.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC-AUC Score\n",
    "\n",
    "**Business Impact:**\n",
    "- Reduces manual spam filtering\n",
    "- Protects users from phishing\n",
    "- Ensures higher productivity and trust in email system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28305a0",
   "metadata": {},
   "source": [
    "### Question 10:\n",
    "**Scenario**: Automatically classify emails as Spam or Not Spam.\n",
    "\n",
    "**Approach**:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Text cleaning: lowercasing, removing punctuation, stopwords, stemming/lemmatization.\n",
    "   - Handling missing values: fill with empty strings or remove rows depending on extent.\n",
    "   - Text vectorization: Use `TfidfVectorizer` for converting emails to numerical format.\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - **Naive Bayes (MultinomialNB)** is preferred due to its effectiveness in text classification tasks, especially with word frequencies.\n",
    "   - **SVM** could be used but may be slower with large, sparse datasets.\n",
    "\n",
    "3. **Handling Class Imbalance**:\n",
    "   - Use SMOTE or class weighting.\n",
    "   - Resample the dataset (oversample minority class or undersample majority class).\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Accuracy, Precision, Recall, F1-Score.\n",
    "   - ROC-AUC for a balanced view of performance.\n",
    "\n",
    "5. **Business Impact**:\n",
    "   - Improved spam detection leads to better productivity.\n",
    "   - Reduces security risk from malicious spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load synthetic text dataset\n",
    "categories = ['sci.crypt', 'talk.politics.misc']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Introduce some missing values artificially\n",
    "X = [x if i % 10 != 0 else None for i, x in enumerate(X)]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def clean_missing_text(X):\n",
    "    return [\" \" if x is None else x for x in X]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clean_text', FunctionTransformer(func=clean_missing_text, validate=False)),\n",
    "    ('tfidf', vectorizer),\n",
    "    ('clf', model)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and predict\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
