{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39da89e6",
      "metadata": {
        "id": "39da89e6"
      },
      "source": [
        "### Question 1: What is the difference between K-Means and Hierarchical Clustering? Provide a use case for each.\n",
        "\n",
        "**Answer:**  \n",
        "- **K-Means**:  \n",
        "  - Partitional clustering algorithm.  \n",
        "  - Requires the number of clusters (k) in advance.  \n",
        "  - Works well with large datasets.  \n",
        "  - Sensitive to initialization and scaling.  \n",
        "  - Example: Customer segmentation in e-commerce.  \n",
        "\n",
        "- **Hierarchical Clustering**:  \n",
        "  - Builds a tree (dendrogram) by either merging (agglomerative) or splitting (divisive) clusters.  \n",
        "  - Does not require pre-specifying the number of clusters.  \n",
        "  - More interpretable but computationally expensive.  \n",
        "  - Example: Document clustering in NLP.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43fa97be",
      "metadata": {
        "id": "43fa97be"
      },
      "source": [
        "### Question 2: Explain the purpose of the Silhouette Score in evaluating clustering algorithms.\n",
        "\n",
        "**Answer:**  \n",
        "The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters.  \n",
        "- Values range from -1 to +1:  \n",
        "  - **+1**: Point is well-clustered.  \n",
        "  - **0**: Point lies on cluster boundary.  \n",
        "  - **-1**: Point is likely in the wrong cluster.  \n",
        "\n",
        "**Purpose**: Helps in evaluating clustering quality and selecting the optimal number of clusters.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b5ccbd",
      "metadata": {
        "id": "a8b5ccbd"
      },
      "source": [
        "### Question 3: What are the core parameters of DBSCAN, and how do they influence the clustering process?\n",
        "\n",
        "**Answer:**  \n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) uses:  \n",
        "- **eps (ε)**: Radius around a data point to search for neighbors. Smaller values → more clusters.  \n",
        "- **min_samples**: Minimum number of points required to form a dense region. Larger values → stricter cluster formation.  \n",
        "\n",
        "These parameters define cluster density. Improper choice may lead to under- or over-clustering.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0c0556",
      "metadata": {
        "id": "ca0c0556"
      },
      "source": [
        "### Question 4: Why is feature scaling important when applying clustering algorithms like K-Means and DBSCAN?\n",
        "\n",
        "**Answer:**  \n",
        "Clustering algorithms rely on distance metrics.  \n",
        "- Without scaling, features with larger ranges dominate distance calculations.  \n",
        "- StandardScaler or MinMaxScaler ensures all features contribute equally.  \n",
        "- Example: In Wine dataset, alcohol % vs magnesium levels → need scaling to avoid bias.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9689afd9",
      "metadata": {
        "id": "9689afd9"
      },
      "source": [
        "### Question 5: What is the Elbow Method in K-Means clustering and how does it help determine the optimal number of clusters?\n",
        "\n",
        "**Answer:**  \n",
        "- Elbow Method plots the **Within-Cluster-Sum of Squares (WCSS)** against number of clusters (k).  \n",
        "- As k increases, WCSS decreases but the marginal gain reduces.  \n",
        "- The \"elbow\" point (sharp bend) indicates optimal k.  \n",
        "- Helps avoid over- or under-clustering.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d072e1",
      "metadata": {
        "id": "c3d072e1"
      },
      "source": [
        "### Question 6: Generate synthetic data using make_blobs(n_samples=300, centers=4), apply KMeans clustering, and visualize the results with cluster centers.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.7, marker='X')\n",
        "plt.title(\"KMeans Clustering with 4 Centers\")\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ea3527",
      "metadata": {
        "id": "14ea3527"
      },
      "source": [
        "### Question 7: Load the Wine dataset, apply StandardScaler, and then train a DBSCAN model. Print the number of clusters found (excluding noise).\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Number of clusters (excluding noise = -1)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters found (excluding noise):\", n_clusters)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a405bca",
      "metadata": {
        "id": "6a405bca"
      },
      "source": [
        "### Question 8: Generate moon-shaped synthetic data using make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in the plot.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate moon-shaped data\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1], c='red', marker='x', label='Outliers')\n",
        "plt.title(\"DBSCAN on Moon-shaped Data with Outliers Highlighted\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d37cf5c",
      "metadata": {
        "id": "0d37cf5c"
      },
      "source": [
        "### Question 9: Load the Wine dataset, reduce it to 2D using PCA, then apply Agglomerative Clustering and visualize the result in 2D with a scatter plot.\n",
        "\n",
        "**Answer (Code + Output):**  \n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow', s=50)\n",
        "plt.title(\"Agglomerative Clustering on Wine Dataset (2D PCA)\")\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51af36d4",
      "metadata": {
        "id": "51af36d4"
      },
      "source": [
        "### Question 10: Real-world e-commerce clustering workflow\n",
        "\n",
        "**Answer:**  \n",
        "Workflow for customer segmentation:  \n",
        "\n",
        "1. **Algorithm Choice**:  \n",
        "   - KMeans for large structured data.  \n",
        "   - DBSCAN if clusters have irregular shapes.  \n",
        "   - Hierarchical for interpretability.  \n",
        "\n",
        "2. **Preprocessing**:  \n",
        "   - Handle missing values (mean/mode imputation).  \n",
        "   - Apply StandardScaler for normalization.  \n",
        "\n",
        "3. **Choosing Number of Clusters**:  \n",
        "   - Elbow Method, Silhouette Score, or Gap Statistics.  \n",
        "\n",
        "4. **Marketing Benefits**:  \n",
        "   - Targeted promotions.  \n",
        "   - Customer retention strategies.  \n",
        "   - Personalized recommendations.  \n",
        "\n",
        "**Example Code:**  \n",
        "```python\n",
        "# Assuming X_customer is preprocessed customer data\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X_customer)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.title(\"Customer Segmentation Clusters\")\n",
        "plt.show()\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}